##	此处略去对各种各样地调用	##

type Scheduler struct {

##  这里定义了一个Scheduler结构体，用以对调度器进行初始化定义，也就是为调度器提供框架

	Cache internalcache.Cache

##  缓存当前集群调度的状态，包含所有节点和已调度的 Pod 的状态，以及一些临时假设的 Pod 状态，优化调度流程

	Extenders []framework.Extender

##  扩展器，用以接入自定义的调度策略。

	NextPod func(logger klog.Logger) (*framework.QueuedPodInfo, error)

##  NextPod是一个阻塞函数，这个函数会将调度器暂时挂起，直到有一个新的 Pod 可供调度时，程序才会继续执行。
##  这个方法本质也还是用以读取下一个Pod，但是可以优化资源消耗。

	FailureHandler FailureHandlerFn

##  错误处理，应该是调出日志一类的。

	SchedulePod func(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (ScheduleResult, error)

##  SchedulePod会尝试将给定的 Pod 调度到节点列表中的某一个节点。如果调度成功会返回建议的主机名称/编号，否则返回 FitError 与错误原因。

	StopEverything <-chan struct{}

##  终止调度器
##  通常，调度器会在一个 Goroutine 中运行调度循环，同时监听 StopEverything 通道。一旦通道关闭，调度器会执行清理操作并停止工作。
##  使用一个通道可以对所有正在监控这个通道的程序进行广播，监测到这个通道关闭后，所有监测到这个变化的程序会在同一时间关闭。
##  与布尔标识（Flag）相比的好处：使用布尔标识的互斥手段需要比较复杂的连锁操作，而通过通道进行关闭可以更加简单且安全的进行处理

	SchedulingQueue internalqueue.SchedulingQueue

##  SchedulingQueue是调度队列，用来存储等待调度的 Pod


	Profiles profile.Map
##  这里是调度配置文件，定义了调度策略和插件

	client clientset.Interface
##  用户接口，用于与 API 服务器通信。

	nodeInfoSnapshot *internalcache.Snapshot
##  生成节点当前的信息快照，用于调度决策。

	percentageOfNodesToScore int32
##  这里是选定需要打分的节点数量，这里的int32只代表整型数据，具体需要在配置文件中指定
##  选择一定数量的节点进行打分，可以优化程序的效率

	nextStartNodeIndex int
##  这个字段会定义在 Kubernetes 调度器调度循环中，遍历Node列表时的开始位置
##  在完成了一次绑定之后，nextStartNodeIndex就会放在在这个绑定的节点上，接着下一次轮寻就会从这个位置开始

	
	logger klog.Logger
##  初始化记录器
##  源程序中有一个提示：创建调度程序时 *必须* 初始化记录器，否则日志记录函数将访问 nil 接收器并报错。

	registeredHandlers []cache.ResourceEventHandlerRegistration
##  这里设置了一个管理和跟踪调度器中所有已注册的资源事件处理程序，会对Pod与节点的变化产生反应。
##  每当一个新的Pod创建请求发出时，调度器通过这些注册的处理程序来执行一系列操作，例如给节点打分，过滤或者绑定。
##  源程序说明中指出这个函数也负责检查所有处理程序是否在调度周期开始之前完成同步。

}


func (sched *Scheduler) applyDefaultHandlers() {

##  这个功能函数将会对调度器进行初始化，确保调度器具有基本的调度逻辑与除错手段。

	sched.SchedulePod = sched.schedulePod
	sched.FailureHandler = sched.handleSchedulingFailure
}



type schedulerOptions struct {

##  这个结构体会对Scheduler结构体进行进一步设置，例如配置调度策略与Pod处理的优先级。


	componentConfigVersion string
	kubeConfig             *restclient.Config
	
	##  这个部分似乎是用来标识调度器的版本信息？而且可以在其他文件中定义
	##  此处的原文是 Overridden by profile level percentageOfNodesToScore if set in v1.

	percentageOfNodesToScore          int32

	##  表示在调度过程中需要评分的节点的百分比。这可以减少调度过程中需要考虑的节点数量，从而优化性能
	##  这个部分在初始化结构中也有

	podInitialBackoffSeconds          int64
	podMaxBackoffSeconds              int64

	##  定义Pod调度失败后，再次进行调度需要的时间范围
	
	podMaxInUnschedulablePodsDuration time.Duration

	##  这里指定了一个Pod可以处于等待队列中存在多长时间
	##  长时间无法进行调度应该会将这个Pod定义为无法调度。

	frameworkOutOfTreeRegistry frameworkruntime.Registry
	profiles                   []schedulerapi.KubeSchedulerProfile
	extenders                  []schedulerapi.Extender
	frameworkCapturer          FrameworkCapturer
	parallelism                int32
	applyDefaultProfile        bool

	##  相关插件的整合
}


type Option func(*schedulerOptions)

##  此处正式开始配置调度文件
##  这是一个函数选项模式（Functional Options Pattern），这里单独只定义了一个指向schedulerOptions结构体的指针。
##  但是之后所有使用这个Option，直接对schedulerOptions内部的相关函数进行定义


type ScheduleResult struct {

##  调度的结果
	
	SuggestedHost string
	##  调度系统为Pod选择的节点。


	EvaluatedNodes int
	FeasibleNodes int
	nominatingInfo *framework.NominatingInfo

	##  EvaluatedNodes: 表示在过滤阶段考虑的节点总数。
	##  FeasibleNodes: 在评估的节点中，实际适合部署Pod的节点数。
	##  nominatingInfo: 提名信息，用于存储调度周期相关的额外信息。

	##  这里记录了在调度过程的过滤阶段以及随后的阶段中，调度器考虑了多少个节点，这些节点的打分情况与这些节点的信息
	##  可以更方便地对调度器的工作情况进行分析
}



## ## ## ##  下列内容都是Option函数，直接对schedulerOptions内部的相关函数进行定义	## ## ## ## 

func WithComponentConfigVersion(apiVersion string) Option {
	return func(o *schedulerOptions) {
		o.componentConfigVersion = apiVersion
	}
}

##	这里设置了调度器配置的版本

// WithKubeConfig sets the kube config for Scheduler.
func WithKubeConfig(cfg *restclient.Config) Option {
	return func(o *schedulerOptions) {
		o.kubeConfig = cfg
	}
}

##	这里设置了与Kubernetes集群通信的相关配置

// WithProfiles sets profiles for Scheduler. By default, there is one profile
// with the name "default-scheduler".
func WithProfiles(p ...schedulerapi.KubeSchedulerProfile) Option {
	return func(o *schedulerOptions) {
		o.profiles = p
		o.applyDefaultProfile = false
	}
}

##	这里配置了调度器的调度配置文件，在默认情况下，这里应该提供一个default-scheduler


func WithParallelism(threads int32) Option {
	return func(o *schedulerOptions) {
		o.parallelism = threads
	}
}

##	这里设置了调度算法的并行执行线程数，初始是16。


func WithPercentageOfNodesToScore(percentageOfNodesToScore *int32) Option {
	return func(o *schedulerOptions) {
		if percentageOfNodesToScore != nil {
			o.percentageOfNodesToScore = *percentageOfNodesToScore
		}
	}
}

##	这里配置了评分阶段时考虑的节点的百分比


func WithFrameworkOutOfTreeRegistry(registry frameworkruntime.Registry) Option {
	return func(o *schedulerOptions) {
		o.frameworkOutOfTreeRegistry = registry
	}
}

##	这里在设置外部插件的注册表

// WithPodInitialBackoffSeconds sets podInitialBackoffSeconds for Scheduler, the default value is 1
func WithPodInitialBackoffSeconds(podInitialBackoffSeconds int64) Option {
	return func(o *schedulerOptions) {
		o.podInitialBackoffSeconds = podInitialBackoffSeconds
	}
}

// WithPodMaxBackoffSeconds sets podMaxBackoffSeconds for Scheduler, the default value is 10
func WithPodMaxBackoffSeconds(podMaxBackoffSeconds int64) Option {
	return func(o *schedulerOptions) {
		o.podMaxBackoffSeconds = podMaxBackoffSeconds
	}
}

##	这里设置了Pod调度失败后重试调度的初始和最大退避时间。

// WithPodMaxInUnschedulablePodsDuration sets podMaxInUnschedulablePodsDuration for PriorityQueue.
func WithPodMaxInUnschedulablePodsDuration(duration time.Duration) Option {
	return func(o *schedulerOptions) {
		o.podMaxInUnschedulablePodsDuration = duration
	}
}

##	这里设置了Pod在优先队列中因不可调度而最大等待时间。

// WithExtenders sets extenders for the Scheduler
func WithExtenders(e ...schedulerapi.Extender) Option {
	return func(o *schedulerOptions) {
		o.extenders = e
	}
}

##	这个部分用来添加扩展器到调度器，这些扩展器可以参与调度决策过程。

##	上面大部分的内容都是对schedulerOptions的客制化定义
##	这个方法不通过一个静态定义来建立schedulerOptions结构，使用会更加灵活

## ## ## ##  至此，Option部分结束	## ## ## ## 

// FrameworkCapturer is used for registering a notify function in building framework.
type FrameworkCapturer func(schedulerapi.KubeSchedulerProfile)

##	该函数提供了一个接口，开发者可以通过这个接口捕获到调度框架的构建过程中的重要信息或状态变更。

// WithBuildFrameworkCapturer sets a notify function for getting buildFramework details.
func WithBuildFrameworkCapturer(fc FrameworkCapturer) Option {
	return func(o *schedulerOptions) {
		o.frameworkCapturer = fc
	}
}

##	该函数同样是一个option函数，指向schedulerOption实例的中的FrameworkCapturer，可以使用户在scheduler建立的过程中添加特殊的操作。
##	注意：这里的操作包括增加日志输出与捕获实时状态等，但是不包括修改调度器的调度逻辑，那个需要通过extenders进行修改


var defaultSchedulerOptions = schedulerOptions{
	percentageOfNodesToScore:          schedulerapi.DefaultPercentageOfNodesToScore,
	podInitialBackoffSeconds:          int64(internalqueue.DefaultPodInitialBackoffDuration.Seconds()),
	podMaxBackoffSeconds:              int64(internalqueue.DefaultPodMaxBackoffDuration.Seconds()),
	podMaxInUnschedulablePodsDuration: internalqueue.DefaultPodMaxInUnschedulablePodsDuration,
	parallelism:                       int32(parallelize.DefaultParallelism),
	// Ideally we would statically set the default profile here, but we can't because
	// creating the default profile may require testing feature gates, which may get
	// set dynamically in tests. Therefore, we delay creating it until New is actually
	// invoked.
	applyDefaultProfile: true,
}

##	这里定义了用户在没有客制化需求时，调度器的默认配置
##	注意：这里构建的调度器并非是可以运行的实体，如果需要创建可以运行在集群的调度器还需要更多的设定
##	至于具体还需要什么设定，可以参照下面的函数：

// New returns a Scheduler
+{

	##	如前文所说，这个函数负责构筑一个调度器实体

	logger := klog.FromContext(ctx)
	##	从集群的上下文中获取日志记录器，用于记录调度器的操作和事件。

	stopEverything := ctx.Done()
	##	检测停止通道，用以处理停止信号

	options := defaultSchedulerOptions
	for _, opt := range opts {
		opt(&options)
	}

	##	这里使用Option函数与循环的结合，获取了defaultSchedulerOptions中的所有变量的初始状态
	##	接着使用opt在option上应用修改
	##	用户可以通过这个方式实现对defaultSchedulerOptions中变量的客制化
	##	注意，这里并不会修改defaultSchedulerOptions本身，只会修改一个副本
	##	虽然在源代码中没有对defaultSchedulerOptions中的参数进行直接的修改
	##	但是由于defaultSchedulerOptions中的参数也通过option函数与SchedulerOptions实例相连
	##	而对SchedulerOptions实例进行客制化的Option函数实际存在
	##	所以这里才会没有与defaultSchedulerOptions直接相连的Option函数


	if options.applyDefaultProfile {
		var versionedCfg configv1.KubeSchedulerConfiguration
		scheme.Scheme.Default(&versionedCfg)
		cfg := schedulerapi.KubeSchedulerConfiguration{}
		if err := scheme.Scheme.Convert(&versionedCfg, &cfg, nil); err != nil {
			return nil, err
		}
		options.profiles = cfg.Profiles
	}

	##	这个函数负责加载和应用 Kubernetes 调度器的默认配置文件
	##	如果需要客制化调度器，这里的相关内容就不会被使用到

	registry := frameworkplugins.NewInTreeRegistry()
	if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil {
		return nil, err
	}

	metrics.Register()

	extenders, err := buildExtenders(logger, options.extenders, options.profiles)
	if err != nil {
		return nil, fmt.Errorf("couldn't build extenders: %w", err)
	}

	podLister := informerFactory.Core().V1().Pods().Lister()
	nodeLister := informerFactory.Core().V1().Nodes().Lister()

	snapshot := internalcache.NewEmptySnapshot()
	metricsRecorder := metrics.NewMetricsAsyncRecorder(1000, time.Second, stopEverything)

	profiles, err := profile.NewMap(ctx, options.profiles, registry, recorderFactory,
		frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion),
		frameworkruntime.WithClientSet(client),
		frameworkruntime.WithKubeConfig(options.kubeConfig),
		frameworkruntime.WithInformerFactory(informerFactory),
		frameworkruntime.WithSnapshotSharedLister(snapshot),
		frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)),
		frameworkruntime.WithParallelism(int(options.parallelism)),
		frameworkruntime.WithExtenders(extenders),
		frameworkruntime.WithMetricsRecorder(metricsRecorder),
	)
	if err != nil {
		return nil, fmt.Errorf("initializing profiles: %v", err)
	}

	if len(profiles) == 0 {
		return nil, errors.New("at least one profile is required")
	}

	preEnqueuePluginMap := make(map[string][]framework.PreEnqueuePlugin)
	queueingHintsPerProfile := make(internalqueue.QueueingHintMapPerProfile)
	for profileName, profile := range profiles {
		preEnqueuePluginMap[profileName] = profile.PreEnqueuePlugins()
		queueingHintsPerProfile[profileName] = buildQueueingHintMap(profile.EnqueueExtensions())
	}

	podQueue := internalqueue.NewSchedulingQueue(
		profiles[options.profiles[0].SchedulerName].QueueSortFunc(),
		informerFactory,
		internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second),
		internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second),
		internalqueue.WithPodLister(podLister),
		internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration),
		internalqueue.WithPreEnqueuePluginMap(preEnqueuePluginMap),
		internalqueue.WithQueueingHintMapPerProfile(queueingHintsPerProfile),
		internalqueue.WithPluginMetricsSamplePercent(pluginMetricsSamplePercent),
		internalqueue.WithMetricsRecorder(*metricsRecorder),
	)

	for _, fwk := range profiles {
		fwk.SetPodNominator(podQueue)
	}

	schedulerCache := internalcache.New(ctx, durationToExpireAssumedPod)

	// Setup cache debugger.
	debugger := cachedebugger.New(nodeLister, podLister, schedulerCache, podQueue)
	debugger.ListenForSignal(ctx)

	sched := &Scheduler{
		Cache:                    schedulerCache,
		client:                   client,
		nodeInfoSnapshot:         snapshot,
		percentageOfNodesToScore: options.percentageOfNodesToScore,
		Extenders:                extenders,
		StopEverything:           stopEverything,
		SchedulingQueue:          podQueue,
		Profiles:                 profiles,
		logger:                   logger,
	}
	sched.NextPod = podQueue.Pop
	sched.applyDefaultHandlers()

	if err = addAllEventHandlers(sched, informerFactory, dynInformerFactory, unionedGVKs(queueingHintsPerProfile)); err != nil {
		return nil, fmt.Errorf("adding event handlers: %w", err)
	}

	return sched, nil
}

// defaultQueueingHintFn is the default queueing hint function.
// It always returns Queue as the queueing hint.
var defaultQueueingHintFn = func(_ klog.Logger, _ *v1.Pod, _, _ interface{}) (framework.QueueingHint, error) {
	return framework.Queue, nil
}

func buildQueueingHintMap(es []framework.EnqueueExtensions) internalqueue.QueueingHintMap {
	queueingHintMap := make(internalqueue.QueueingHintMap)
	for _, e := range es {
		events := e.EventsToRegister()

		// This will happen when plugin registers with empty events, it's usually the case a pod
		// will become reschedulable only for self-update, e.g. schedulingGates plugin, the pod
		// will enter into the activeQ via priorityQueue.Update().
		if len(events) == 0 {
			continue
		}

		// Note: Rarely, a plugin implements EnqueueExtensions but returns nil.
		// We treat it as: the plugin is not interested in any event, and hence pod failed by that plugin
		// cannot be moved by any regular cluster event.
		// So, we can just ignore such EventsToRegister here.

		registerNodeAdded := false
		registerNodeTaintUpdated := false
		for _, event := range events {
			fn := event.QueueingHintFn
			if fn == nil || !utilfeature.DefaultFeatureGate.Enabled(features.SchedulerQueueingHints) {
				fn = defaultQueueingHintFn
			}

			if event.Event.Resource == framework.Node {
				if event.Event.ActionType&framework.Add != 0 {
					registerNodeAdded = true
				}
				if event.Event.ActionType&framework.UpdateNodeTaint != 0 {
					registerNodeTaintUpdated = true
				}
			}

			queueingHintMap[event.Event] = append(queueingHintMap[event.Event], &internalqueue.QueueingHintFunction{
				PluginName:     e.Name(),
				QueueingHintFn: fn,
			})
		}
		if registerNodeAdded && !registerNodeTaintUpdated {
			// Temporally fix for the issue https://github.com/kubernetes/kubernetes/issues/109437
			// NodeAdded QueueingHint isn't always called because of preCheck.
			// It's definitely not something expected for plugin developers,
			// and registering UpdateNodeTaint event is the only mitigation for now.
			//
			// So, here registers UpdateNodeTaint event for plugins that has NodeAdded event, but don't have UpdateNodeTaint event.
			// It has a bad impact for the requeuing efficiency though, a lot better than some Pods being stuch in the
			// unschedulable pod pool.
			// This behavior will be removed when we remove the preCheck feature.
			// See: https://github.com/kubernetes/kubernetes/issues/110175
			queueingHintMap[framework.ClusterEvent{Resource: framework.Node, ActionType: framework.UpdateNodeTaint}] =
				append(queueingHintMap[framework.ClusterEvent{Resource: framework.Node, ActionType: framework.UpdateNodeTaint}],
					&internalqueue.QueueingHintFunction{
						PluginName:     e.Name(),
						QueueingHintFn: defaultQueueingHintFn,
					},
				)
		}
	}
	return queueingHintMap
}

// Run begins watching and scheduling. It starts scheduling and blocked until the context is done.
func (sched *Scheduler) Run(ctx context.Context) {
	logger := klog.FromContext(ctx)
	sched.SchedulingQueue.Run(logger)

	// We need to start scheduleOne loop in a dedicated goroutine,
	// because scheduleOne function hangs on getting the next item
	// from the SchedulingQueue.
	// If there are no new pods to schedule, it will be hanging there
	// and if done in this goroutine it will be blocking closing
	// SchedulingQueue, in effect causing a deadlock on shutdown.
	go wait.UntilWithContext(ctx, sched.ScheduleOne, 0)

	<-ctx.Done()
	sched.SchedulingQueue.Close()

	// If the plugins satisfy the io.Closer interface, they are closed.
	err := sched.Profiles.Close()
	if err != nil {
		logger.Error(err, "Failed to close plugins")
	}
}

// NewInformerFactory creates a SharedInformerFactory and initializes a scheduler specific
// in-place podInformer.
func NewInformerFactory(cs clientset.Interface, resyncPeriod time.Duration) informers.SharedInformerFactory {
	informerFactory := informers.NewSharedInformerFactory(cs, resyncPeriod)
	informerFactory.InformerFor(&v1.Pod{}, newPodInformer)
	return informerFactory
}

func buildExtenders(logger klog.Logger, extenders []schedulerapi.Extender, profiles []schedulerapi.KubeSchedulerProfile) ([]framework.Extender, error) {
	var fExtenders []framework.Extender
	if len(extenders) == 0 {
		return nil, nil
	}

	var ignoredExtendedResources []string
	var ignorableExtenders []framework.Extender
	for i := range extenders {
		logger.V(2).Info("Creating extender", "extender", extenders[i])
		extender, err := NewHTTPExtender(&extenders[i])
		if err != nil {
			return nil, err
		}
		if !extender.IsIgnorable() {
			fExtenders = append(fExtenders, extender)
		} else {
			ignorableExtenders = append(ignorableExtenders, extender)
		}
		for _, r := range extenders[i].ManagedResources {
			if r.IgnoredByScheduler {
				ignoredExtendedResources = append(ignoredExtendedResources, r.Name)
			}
		}
	}
	// place ignorable extenders to the tail of extenders
	fExtenders = append(fExtenders, ignorableExtenders...)

	// If there are any extended resources found from the Extenders, append them to the pluginConfig for each profile.
	// This should only have an effect on ComponentConfig, where it is possible to configure Extenders and
	// plugin args (and in which case the extender ignored resources take precedence).
	if len(ignoredExtendedResources) == 0 {
		return fExtenders, nil
	}

	for i := range profiles {
		prof := &profiles[i]
		var found = false
		for k := range prof.PluginConfig {
			if prof.PluginConfig[k].Name == noderesources.Name {
				// Update the existing args
				pc := &prof.PluginConfig[k]
				args, ok := pc.Args.(*schedulerapi.NodeResourcesFitArgs)
				if !ok {
					return nil, fmt.Errorf("want args to be of type NodeResourcesFitArgs, got %T", pc.Args)
				}
				args.IgnoredResources = ignoredExtendedResources
				found = true
				break
			}
		}
		if !found {
			return nil, fmt.Errorf("can't find NodeResourcesFitArgs in plugin config")
		}
	}
	return fExtenders, nil
}

type FailureHandlerFn func(ctx context.Context, fwk framework.Framework, podInfo *framework.QueuedPodInfo, status *framework.Status, nominatingInfo *framework.NominatingInfo, start time.Time)

func unionedGVKs(queueingHintsPerProfile internalqueue.QueueingHintMapPerProfile) map[framework.GVK]framework.ActionType {
	gvkMap := make(map[framework.GVK]framework.ActionType)
	for _, queueingHints := range queueingHintsPerProfile {
		for evt := range queueingHints {
			if _, ok := gvkMap[evt.Resource]; ok {
				gvkMap[evt.Resource] |= evt.ActionType
			} else {
				gvkMap[evt.Resource] = evt.ActionType
			}
		}
	}
	return gvkMap
}

// newPodInformer creates a shared index informer that returns only non-terminal pods.
// The PodInformer allows indexers to be added, but note that only non-conflict indexers are allowed.
func newPodInformer(cs clientset.Interface, resyncPeriod time.Duration) cache.SharedIndexInformer {
	selector := fmt.Sprintf("status.phase!=%v,status.phase!=%v", v1.PodSucceeded, v1.PodFailed)
	tweakListOptions := func(options *metav1.ListOptions) {
		options.FieldSelector = selector
	}
	informer := coreinformers.NewFilteredPodInformer(cs, metav1.NamespaceAll, resyncPeriod, cache.Indexers{}, tweakListOptions)

	// Dropping `.metadata.managedFields` to improve memory usage.
	// The Extract workflow (i.e. `ExtractPod`) should be unused.
	trim := func(obj interface{}) (interface{}, error) {
		if accessor, err := meta.Accessor(obj); err == nil {
			if accessor.GetManagedFields() != nil {
				accessor.SetManagedFields(nil)
			}
		}
		return obj, nil
	}
	informer.SetTransform(trim)
	return informer
}
