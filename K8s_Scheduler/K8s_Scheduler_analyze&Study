##	此处略去对各种各样地调用	##

type Scheduler struct {

##  这里定义了一个Scheduler结构体，用以对调度器进行初始化定义，也就是为调度器提供框架

	Cache internalcache.Cache

##  缓存当前集群调度的状态，包含所有节点和已调度的 Pod 的状态，以及一些临时假设的 Pod 状态，优化调度流程

	Extenders []framework.Extender

##  扩展器，用以接入自定义的调度策略。

	NextPod func(logger klog.Logger) (*framework.QueuedPodInfo, error)

##  NextPod是一个阻塞函数，这个函数会将调度器暂时挂起，直到有一个新的 Pod 可供调度时，程序才会继续执行。
##  这个方法本质也还是用以读取下一个Pod，但是可以优化资源消耗。

	FailureHandler FailureHandlerFn

##  错误处理，应该是调出日志一类的。

	SchedulePod func(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (ScheduleResult, error)

##  SchedulePod会尝试将给定的 Pod 调度到节点列表中的某一个节点。如果调度成功会返回建议的主机名称/编号，否则返回 FitError 与错误原因。

	StopEverything <-chan struct{}

##  终止调度器
##  通常，调度器会在一个 Goroutine 中运行调度循环，同时监听 StopEverything 通道。一旦通道关闭，调度器会执行清理操作并停止工作。
##  使用一个通道可以对所有正在监控这个通道的程序进行广播，监测到这个通道关闭后，所有监测到这个变化的程序会在同一时间关闭。
##  与布尔标识（Flag）相比的好处：使用布尔标识的互斥手段需要比较复杂的连锁操作，而通过通道进行关闭可以更加简单且安全的进行处理

	SchedulingQueue internalqueue.SchedulingQueue

##  SchedulingQueue是调度队列，用来存储等待调度的 Pod


	Profiles profile.Map
##  这里是调度配置文件，定义了调度策略和插件

	client clientset.Interface
##  用户接口，用于与 API 服务器通信。

	nodeInfoSnapshot *internalcache.Snapshot
##  生成节点当前的信息快照，用于调度决策。

	percentageOfNodesToScore int32
##  这里是选定需要打分的节点数量，这里的int32只代表整型数据，具体需要在配置文件中指定
##  选择一定数量的节点进行打分，可以优化程序的效率

	nextStartNodeIndex int
##  这个字段会定义在 Kubernetes 调度器调度循环中，遍历Node列表时的开始位置
##  在完成了一次绑定之后，nextStartNodeIndex就会放在在这个绑定的节点上，接着下一次轮寻就会从这个位置开始

	
	logger klog.Logger
##  初始化记录器
##  源程序中有一个提示：创建调度程序时 *必须* 初始化记录器，否则日志记录函数将访问 nil 接收器并报错。

	registeredHandlers []cache.ResourceEventHandlerRegistration
##  这里设置了一个管理和跟踪调度器中所有已注册的资源事件处理程序，会对Pod与节点的变化产生反应。
##  每当一个新的Pod创建请求发出时，调度器通过这些注册的处理程序来执行一系列操作，例如给节点打分，过滤或者绑定。
##  源程序说明中指出这个函数也负责检查所有处理程序是否在调度周期开始之前完成同步。

}


func (sched *Scheduler) applyDefaultHandlers() {

##  这个功能函数将会对调度器进行初始化，确保调度器具有基本的调度逻辑与除错手段。

	sched.SchedulePod = sched.schedulePod
	sched.FailureHandler = sched.handleSchedulingFailure
}



type schedulerOptions struct {

##  这个结构体会对Scheduler结构体进行进一步设置，例如配置调度策略与Pod处理的优先级。


	componentConfigVersion string
	kubeConfig             *restclient.Config
	
	##  这个部分似乎是用来标识调度器的版本信息？而且可以在其他文件中定义
	##  此处的原文是 Overridden by profile level percentageOfNodesToScore if set in v1.

	percentageOfNodesToScore          int32

	##  表示在调度过程中需要评分的节点的百分比。这可以减少调度过程中需要考虑的节点数量，从而优化性能
	##  这个部分在初始化结构中也有

	podInitialBackoffSeconds          int64
	podMaxBackoffSeconds              int64

	##  定义Pod调度失败后，再次进行调度需要的时间范围
	
	podMaxInUnschedulablePodsDuration time.Duration

	##  这里指定了一个Pod可以处于等待队列中存在多长时间
	##  长时间无法进行调度应该会将这个Pod定义为无法调度。

	frameworkOutOfTreeRegistry frameworkruntime.Registry
	profiles                   []schedulerapi.KubeSchedulerProfile
	extenders                  []schedulerapi.Extender
	frameworkCapturer          FrameworkCapturer
	parallelism                int32
	applyDefaultProfile        bool

	##  相关插件的整合
}


type Option func(*schedulerOptions)

##  此处正式开始配置调度文件
##  这是一个函数选项模式（Functional Options Pattern），这里单独只定义了一个指向schedulerOptions结构体的指针。
##  但是之后所有使用这个Option，直接对schedulerOptions内部的相关函数进行定义


type ScheduleResult struct {

##  调度的结果
	
	SuggestedHost string
	##  调度系统为Pod选择的节点。


	EvaluatedNodes int
	FeasibleNodes int
	nominatingInfo *framework.NominatingInfo

	##  EvaluatedNodes: 表示在过滤阶段考虑的节点总数。
	##  FeasibleNodes: 在评估的节点中，实际适合部署Pod的节点数。
	##  nominatingInfo: 提名信息，用于存储调度周期相关的额外信息。

	##  这里记录了在调度过程的过滤阶段以及随后的阶段中，调度器考虑了多少个节点，这些节点的打分情况与这些节点的信息
	##  可以更方便地对调度器的工作情况进行分析
}



## ## ## ##  下列内容都是Option函数，直接对schedulerOptions内部的相关函数进行定义	## ## ## ## 

func WithComponentConfigVersion(apiVersion string) Option {
	return func(o *schedulerOptions) {
		o.componentConfigVersion = apiVersion
	}
}

##	这里设置了调度器配置的版本

// WithKubeConfig sets the kube config for Scheduler.
func WithKubeConfig(cfg *restclient.Config) Option {
	return func(o *schedulerOptions) {
		o.kubeConfig = cfg
	}
}

##	这里设置了与Kubernetes集群通信的相关配置

// WithProfiles sets profiles for Scheduler. By default, there is one profile
// with the name "default-scheduler".
func WithProfiles(p ...schedulerapi.KubeSchedulerProfile) Option {
	return func(o *schedulerOptions) {
		o.profiles = p
		o.applyDefaultProfile = false
	}
}

##	这里配置了调度器的调度配置文件，在默认情况下，这里应该提供一个default-scheduler


func WithParallelism(threads int32) Option {
	return func(o *schedulerOptions) {
		o.parallelism = threads
	}
}

##	这里设置了调度算法的并行执行线程数，初始是16。


func WithPercentageOfNodesToScore(percentageOfNodesToScore *int32) Option {
	return func(o *schedulerOptions) {
		if percentageOfNodesToScore != nil {
			o.percentageOfNodesToScore = *percentageOfNodesToScore
		}
	}
}

##	这里配置了评分阶段时考虑的节点的百分比


func WithFrameworkOutOfTreeRegistry(registry frameworkruntime.Registry) Option {
	return func(o *schedulerOptions) {
		o.frameworkOutOfTreeRegistry = registry
	}
}

##	这里在设置外部插件的注册表

// WithPodInitialBackoffSeconds sets podInitialBackoffSeconds for Scheduler, the default value is 1
func WithPodInitialBackoffSeconds(podInitialBackoffSeconds int64) Option {
	return func(o *schedulerOptions) {
		o.podInitialBackoffSeconds = podInitialBackoffSeconds
	}
}

// WithPodMaxBackoffSeconds sets podMaxBackoffSeconds for Scheduler, the default value is 10
func WithPodMaxBackoffSeconds(podMaxBackoffSeconds int64) Option {
	return func(o *schedulerOptions) {
		o.podMaxBackoffSeconds = podMaxBackoffSeconds
	}
}

##	这里设置了Pod调度失败后重试调度的初始和最大退避时间。

// WithPodMaxInUnschedulablePodsDuration sets podMaxInUnschedulablePodsDuration for PriorityQueue.
func WithPodMaxInUnschedulablePodsDuration(duration time.Duration) Option {
	return func(o *schedulerOptions) {
		o.podMaxInUnschedulablePodsDuration = duration
	}
}

##	这里设置了Pod在优先队列中因不可调度而最大等待时间。

// WithExtenders sets extenders for the Scheduler
func WithExtenders(e ...schedulerapi.Extender) Option {
	return func(o *schedulerOptions) {
		o.extenders = e
	}
}

##	这个部分用来添加扩展器到调度器，这些扩展器可以参与调度决策过程。

##	上面大部分的内容都是对schedulerOptions的客制化定义
##	这个方法不通过一个静态定义来建立schedulerOptions结构，使用会更加灵活

## ## ## ##  至此，Option部分结束	## ## ## ## 

// FrameworkCapturer is used for registering a notify function in building framework.
type FrameworkCapturer func(schedulerapi.KubeSchedulerProfile)

##	该函数提供了一个接口，开发者可以通过这个接口捕获到调度框架的构建过程中的重要信息或状态变更。

// WithBuildFrameworkCapturer sets a notify function for getting buildFramework details.
func WithBuildFrameworkCapturer(fc FrameworkCapturer) Option {
	return func(o *schedulerOptions) {
		o.frameworkCapturer = fc
	}
}

##	该函数同样是一个option函数，指向schedulerOption实例的中的FrameworkCapturer，可以使用户在scheduler建立的过程中添加特殊的操作。
##	注意：这里的操作包括增加日志输出与捕获实时状态等，但是不包括修改调度器的调度逻辑，那个需要通过extenders进行修改


var defaultSchedulerOptions = schedulerOptions{
	percentageOfNodesToScore:          schedulerapi.DefaultPercentageOfNodesToScore,
	podInitialBackoffSeconds:          int64(internalqueue.DefaultPodInitialBackoffDuration.Seconds()),
	podMaxBackoffSeconds:              int64(internalqueue.DefaultPodMaxBackoffDuration.Seconds()),
	podMaxInUnschedulablePodsDuration: internalqueue.DefaultPodMaxInUnschedulablePodsDuration,
	parallelism:                       int32(parallelize.DefaultParallelism),
	// Ideally we would statically set the default profile here, but we can't because
	// creating the default profile may require testing feature gates, which may get
	// set dynamically in tests. Therefore, we delay creating it until New is actually
	// invoked.
	applyDefaultProfile: true,
}

##	这里定义了用户在没有客制化需求时，调度器的默认配置
##	注意：这里构建的调度器并非是可以运行的实体，如果需要创建可以运行在集群的调度器还需要更多的设定
##	至于具体还需要什么设定，可以参照下面的函数：

// New returns a Scheduler

func New(ctx context.Context,
	client clientset.Interface,
	informerFactory informers.SharedInformerFactory,
	dynInformerFactory dynamicinformer.DynamicSharedInformerFactory,
	recorderFactory profile.RecorderFactory,
	opts ...Option) (*Scheduler, error) {

	##	如前文所说，这个函数负责构筑一个调度器实体

	logger := klog.FromContext(ctx)
	##	从集群的上下文中获取日志记录器，用于记录调度器的操作和事件。

	stopEverything := ctx.Done()
	##	检测停止通道，用以处理停止信号

	options := defaultSchedulerOptions
	for _, opt := range opts {
		opt(&options)
	}

	##	这里使用Option函数与循环的结合，获取了defaultSchedulerOptions中的所有变量的初始状态
	##	接着使用opt在option上应用修改
	##	用户可以通过这个方式实现对defaultSchedulerOptions中变量的客制化
	##	注意，这里并不会修改defaultSchedulerOptions本身，只会修改一个副本
	##	虽然在源代码中没有对defaultSchedulerOptions中的参数进行直接的修改
	##	但是由于defaultSchedulerOptions中的参数也通过option函数与SchedulerOptions实例相连
	##	而对SchedulerOptions实例进行客制化的Option函数实际存在
	##	所以这里才会没有与defaultSchedulerOptions直接相连的Option函数


	if options.applyDefaultProfile {
		var versionedCfg configv1.KubeSchedulerConfiguration
		scheme.Scheme.Default(&versionedCfg)
		cfg := schedulerapi.KubeSchedulerConfiguration{}
		if err := scheme.Scheme.Convert(&versionedCfg, &cfg, nil); err != nil {
			return nil, err
		}
		options.profiles = cfg.Profiles
	}

	##	这个函数负责加载和应用 Kubernetes 调度器的默认配置文件
	##	如果需要客制化调度器，这里的相关内容就不会被使用到

	registry := frameworkplugins.NewInTreeRegistry()
	if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil {
		return nil, err
	}

	metrics.Register()

	##	这里创建一个新的插件注册表，里面整合了所有来自内部（来自frameworkplugins.NewInTreeRegistry）
	##	与外部的插件（来自registry.Merge(options.frameworkOutOfTreeRegistry)）
	##	接着使用metrics.Register()注册了一组调度器的运行时监控指标，，用以确认所有插件的工作性能


	extenders, err := buildExtenders(logger, options.extenders, options.profiles)
	if err != nil {
		return nil, fmt.Errorf("couldn't build extenders: %w", err)
	}

	##	拓展器设置，可以通过这里导入一个自定义的外部调度器
	##	这里接入的外部调度器不需要修改原本的调度逻辑
	##	如果接入出现问题也会报错

	podLister := informerFactory.Core().V1().Pods().Lister()
	nodeLister := informerFactory.Core().V1().Nodes().Lister()

	##	这两个Lister负责从本地缓存中检索所有 Pod 和 Node 的信息。
	##	这些缓存会一直检索所有 Pod 和 Node 的更新信息并自动更新，以保证数据的时效性和准确性
	##	而调度器访问这些信息不需要通过API，能够加快调度器的效率

	snapshot := internalcache.NewEmptySnapshot()

	##	这里创建了一组创建了一个新的空的快照对象
	##	里面储存了所有节点的信息以及这些节点上的 Pod 分布情况
	##	这个快照也会在一个调度周期后进行更新，可以帮助调度器做出决策


	metricsRecorder := metrics.NewMetricsAsyncRecorder(1000, time.Second, stopEverything)

	##	这里创建了一个新的记录器，用于监控调度器的性能
	##	可以根据这些指标优化调度器的性能


	profiles, err := profile.NewMap(ctx, options.profiles, registry, recorderFactory,
		frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion),
		frameworkruntime.WithClientSet(client),
		frameworkruntime.WithKubeConfig(options.kubeConfig),
		frameworkruntime.WithInformerFactory(informerFactory),
		frameworkruntime.WithSnapshotSharedLister(snapshot),
		frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)),
		frameworkruntime.WithParallelism(int(options.parallelism)),
		frameworkruntime.WithExtenders(extenders),
		frameworkruntime.WithMetricsRecorder(metricsRecorder),
	)

	##	在初始化 scheduler 时，会创建一个 profile，profile是关于 scheduler 调度配置相关的定义
	##	关于 profile 的实现，它作为 KubeSchedulerProfile，也就是在调度器生成时作为 yaml 传入的配置
	##	profile.NewMap 就是根据给定的配置来构建framework，因为配置可能是同时存在多个


	if err != nil {
		return nil, fmt.Errorf("initializing profiles: %v", err)
	}

	if len(profiles) == 0 {
		return nil, errors.New("at least one profile is required")
	}

	##	这里调用一个名为profile的实体中的NewMap的函数，为调度器映射了一些基础设置
	##	其中包含了调度器的多个配置文件（profiles）。
	##	这些配置文件包含了调度器运行所必需的各种调度策略和规则。
	##	如果这些参数的映像设置成功，将返回一个充满有效调度配置文件的 profiles 映射，err就是nil。
	##	如果初始化结束都没有任何配置文件，调度器无法正常工作
	##	会返回错误信息"at least one profile is required"

	preEnqueuePluginMap := make(map[string][]framework.PreEnqueuePlugin)

	##	这里创建了一个映射，键值为配置文件名（profile name），值为实现了 PreEnqueuePlugin 接口的插件列表。
	##	这个接口的插件在 Pod 进入调度队列之前执行
	##	负责入队前的合规性和适用性检查，确保只有合适的 Pods 能够进入调度过程。

	queueingHintsPerProfile := make(internalqueue.QueueingHintMapPerProfile)

	##	这里创建了另一个映射，用于存储每个配置文件对应的队列提示。
	##	里面包含从插件生成的提示信息，这些信息指导调度器如何处理队列操作。
	##	可以根据当前集群状态、Pods 的特定特征或优先级来调整其入队行为。


	for profileName, profile := range profiles {
		preEnqueuePluginMap[profileName] = profile.PreEnqueuePlugins()
		queueingHintsPerProfile[profileName] = buildQueueingHintMap(profile.EnqueueExtensions())
	}

	##	这里使用遍历为每个profile设置之前两个负责Pod调度的映射

	podQueue := internalqueue.NewSchedulingQueue(
		profiles[options.profiles[0].SchedulerName].QueueSortFunc(),
		##	从首个配置文件中获取队列的排序函数
		informerFactory,
		##	传递 用于创建和管理 Kubernetes 资源的 Informer Factory
		internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second),
		internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second),
		internalqueue.WithPodLister(podLister),
		internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration),
		internalqueue.WithPreEnqueuePluginMap(preEnqueuePluginMap),
		internalqueue.WithQueueingHintMapPerProfile(queueingHintsPerProfile),
		internalqueue.WithPluginMetricsSamplePercent(pluginMetricsSamplePercent),
		internalqueue.WithMetricsRecorder(*metricsRecorder),
		##	这些配置项在前文的程序中都有过调用，它们完善了PodQueue的功能
	)

	##	这里设置了调度队列，是调度器的重要组件之一
	##	内容包括管理等待被调度的 Pods，包括如何排序、如何处理无法立即调度的 Pods，以及如何反馈调度尝试的结果。

	for _, fwk := range profiles {
		fwk.SetPodNominator(podQueue)
	}

	##	这里将所有的PodQueue与profile结合起来
	##	确保每个调度配置文件能够正确地访问和操作调度队列。
	##	SetPodNominator 方法允许每个调度框架通过提供的 podQueue 访问和管理 Pods 的队列与提名。
	##	Pod Nominator 是一个组件，负责记录和管理在调度过程中可能需要被重新调度的 Pods。

	schedulerCache := internalcache.New(ctx, durationToExpireAssumedPod)

	##	初始化调度器缓存，它负责管理 Pods 和节点信息，并提供了一个Pod 过期机制
	##	durationToExpireAssumedPod 指定了一个Pod在缓存中可以存在的最大时长
	##	如果这段时间内 Pod 没有被绑定到任何一个节点，则它会从缓存中过期并被移除。

	// Setup cache debugger.
	debugger := cachedebugger.New(nodeLister, podLister, schedulerCache, podQueue)
	debugger.ListenForSignal(ctx)

	##	设置一个缓存调试器，负责监控刚才设置的schedulerCache
	##	同时其中还集成了对节点和 Pods 的监控
	##	可以通过 nodeLister 和 podLister 能够访问最新的集群数据

	sched := &Scheduler{
		Cache:                    schedulerCache,
		client:                   client,
		nodeInfoSnapshot:         snapshot,
		percentageOfNodesToScore: options.percentageOfNodesToScore,
		Extenders:                extenders,
		StopEverything:           stopEverything,
		SchedulingQueue:          podQueue,
		Profiles:                 profiles,
		logger:                   logger,
	}

	##	这段代码使用了前文提供的功能函数创建和配置一个 Scheduler 实体

	sched.NextPod = podQueue.Pop
	
	##	这里设置了调度器选择下一个Pod的方法为podQueue.Pop
	##	这个方法是按照排序顺序依次读取Pod，适用面很广
	
	sched.applyDefaultHandlers()

	##	这里设置了一些默认的处理方法
	##	例如当一个 Pod 无法被成功调度，一个默认的处理程序可能会尝试将 Pod 重新加入到调度队列，或记录相关的错误信息。

	if err = addAllEventHandlers(sched, informerFactory, dynInformerFactory, unionedGVKs(queueingHintsPerProfile)); err != nil {
		return nil, fmt.Errorf("adding event handlers: %w", err)
	}

	##	这个函数首先使用 addAllEventHandlers 为调度器 (sched) 添加各种事件处理器
	##	这些处理器使用基于 informerFactory 和 dynInformerFactory 创建的 Informers 监听 Kubernetes 资源的变化
	##	接着程序会立即检查函数调用的结果是否返回错误，如果存在错误则会返回错误信息

	##	这样确保了只有在所有事件处理器成功添加之后，调度器才会继续执行后续操作

	return sched, nil
}



// defaultQueueingHintFn is the default queueing hint function.
// It always returns Queue as the queueing hint.
var defaultQueueingHintFn = func(_ klog.Logger, _ *v1.Pod, _, _ interface{}) (framework.QueueingHint, error) {
	return framework.Queue, nil
}

##	这是 K8s 调度器中的一个默认队列提示函数
##	作用是提供一个基础的队列提示，指导调度器如何将 Pods 排入调度队列
##	似乎无论Pod与节点是什么情况，Pod都会以一个固定顺序进行调度
##	我猜想这是用于处理PodQueue无法处理的Pod，将那些Pod强制绑定上节点进行执行

func buildQueueingHintMap(es []framework.EnqueueExtensions) internalqueue.QueueingHintMap {
	queueingHintMap := make(internalqueue.QueueingHintMap)
	for _, e := range es {
		events := e.EventsToRegister()

		// This will happen when plugin registers with empty events, it's usually the case a pod
		// will become reschedulable only for self-update, e.g. schedulingGates plugin, the pod
		// will enter into the activeQ via priorityQueue.Update().
		if len(events) == 0 {
			continue
		}

		// Note: Rarely, a plugin implements EnqueueExtensions but returns nil.
		// We treat it as: the plugin is not interested in any event, and hence pod failed by that plugin
		// cannot be moved by any regular cluster event.
		// So, we can just ignore such EventsToRegister here.

		registerNodeAdded := false
		registerNodeTaintUpdated := false
		for _, event := range events {
			fn := event.QueueingHintFn
			if fn == nil || !utilfeature.DefaultFeatureGate.Enabled(features.SchedulerQueueingHints) {
				fn = defaultQueueingHintFn
			}

			if event.Event.Resource == framework.Node {
				if event.Event.ActionType&framework.Add != 0 {
					registerNodeAdded = true
				}
				if event.Event.ActionType&framework.UpdateNodeTaint != 0 {
					registerNodeTaintUpdated = true
				}
			}

			queueingHintMap[event.Event] = append(queueingHintMap[event.Event], &internalqueue.QueueingHintFunction{
				PluginName:     e.Name(),
				QueueingHintFn: fn,
			})
		}
		if registerNodeAdded && !registerNodeTaintUpdated {
			// Temporally fix for the issue https://github.com/kubernetes/kubernetes/issues/109437
			// NodeAdded QueueingHint isn't always called because of preCheck.
			// It's definitely not something expected for plugin developers,
			// and registering UpdateNodeTaint event is the only mitigation for now.
			//
			// So, here registers UpdateNodeTaint event for plugins that has NodeAdded event, but don't have UpdateNodeTaint event.
			// It has a bad impact for the requeuing efficiency though, a lot better than some Pods being stuch in the
			// unschedulable pod pool.
			// This behavior will be removed when we remove the preCheck feature.
			// See: https://github.com/kubernetes/kubernetes/issues/110175
			queueingHintMap[framework.ClusterEvent{Resource: framework.Node, ActionType: framework.UpdateNodeTaint}] =
				append(queueingHintMap[framework.ClusterEvent{Resource: framework.Node, ActionType: framework.UpdateNodeTaint}],
					&internalqueue.QueueingHintFunction{
						PluginName:     e.Name(),
						QueueingHintFn: defaultQueueingHintFn,
					},
				)
		}
	}
	return queueingHintMap
}

##	这里设置了一个buildQueueingHintMap 函数，建立事件与队列提示函数的映射
##	负责根据集群事件实时处理新加入PodQueue（调度队列）的 Pod 的处理方式
##	也可以动态响应集群中的变化，如节点的添加或更新，从而更智能地管理待调度的 Pods。

// Run begins watching and scheduling. It starts scheduling and blocked until the context is done.
func (sched *Scheduler) Run(ctx context.Context) {
	logger := klog.FromContext(ctx)
	sched.SchedulingQueue.Run(logger)

	// We need to start scheduleOne loop in a dedicated goroutine,
	// because scheduleOne function hangs on getting the next item
	// from the SchedulingQueue.
	// If there are no new pods to schedule, it will be hanging there
	// and if done in this goroutine it will be blocking closing
	// SchedulingQueue, in effect causing a deadlock on shutdown.
	go wait.UntilWithContext(ctx, sched.ScheduleOne, 0)

	<-ctx.Done()
	sched.SchedulingQueue.Close()

	// If the plugins satisfy the io.Closer interface, they are closed.
	err := sched.Profiles.Close()
	if err != nil {
		logger.Error(err, "Failed to close plugins")
	}
}

##	这段代码定义了 Kubernetes 调度器的 Run 方法，该方法负责启动调度器并管理其整个生命周期
##	这个方法包括启动调度队列、处理调度逻辑，以及正确地处理调度器停止时的清理工作。


// NewInformerFactory creates a SharedInformerFactory and initializes a scheduler specific
// in-place podInformer.
func NewInformerFactory(cs clientset.Interface, resyncPeriod time.Duration) informers.SharedInformerFactory {
	informerFactory := informers.NewSharedInformerFactory(cs, resyncPeriod)
	informerFactory.InformerFor(&v1.Pod{}, newPodInformer)
	return informerFactory
}

##	这段代码定义了一个名为 NewInformerFactory 的函数，创建一个 SharedInformerFactory 实例 
##	用于创建和管理 Informer。并使用这些Informer用来监听 Kubernetes API 服务器上资源状态变化。
##	并设置一个直接在调度器中初始化和配置的podInformer
##	这是一种特定类型的 Informer，专门用于监控和响应与 Pod 相关的事件

func buildExtenders(logger klog.Logger, extenders []schedulerapi.Extender, profiles []schedulerapi.KubeSchedulerProfile) ([]framework.Extender, error) {
	var fExtenders []framework.Extender
	if len(extenders) == 0 {
		return nil, nil
	}

	##	这里首先检查了extenders 列表是否为空
	##	如果为空则表示示没有扩展器要处理

	var ignoredExtendedResources []string
	var ignorableExtenders []framework.Extender

	for i := range extenders {
		logger.V(2).Info("Creating extender", "extender", extenders[i])
		extender, err := NewHTTPExtender(&extenders[i])
		if err != nil {
			return nil, err
		}
		if !extender.IsIgnorable() {
			fExtenders = append(fExtenders, extender)
		} else {
			ignorableExtenders = append(ignorableExtenders, extender)
		}
		for _, r := range extenders[i].ManagedResources {
			if r.IgnoredByScheduler {
				ignoredExtendedResources = append(ignoredExtendedResources, r.Name)
			}
		}
	}

	##	这里遍历了所有拓展调度器，将它们分为可忽略与不可忽略的两类
	##	可忽略类的在调度过程中如果非必要可以不使用，会做上ignorableExtenders的标志
	##	不可忽略则是必须在调度过程中使用
	##	接着将所有带有ignorableExtenders标签的调度器都会被放入ignoredExtendedResources数组
	##	这些数组内的调度器将不会影响调度器的最终结果

	fExtenders = append(fExtenders, ignorableExtenders...)

	##	这里是一个append函数
	##	作用是将所有的 ignorableExtenders 放置到 fExtenders 的末尾

	if len(ignoredExtendedResources) == 0 {
		return fExtenders, nil
	}

	##	如果没有可以被忽略的调度器，程序会直接返回不可忽略

	for i := range profiles {
		prof := &profiles[i]
		var found = false
		for k := range prof.PluginConfig {
			if prof.PluginConfig[k].Name == noderesources.Name {
				// Update the existing args
				pc := &prof.PluginConfig[k]
				args, ok := pc.Args.(*schedulerapi.NodeResourcesFitArgs)
				if !ok {
					return nil, fmt.Errorf("want args to be of type NodeResourcesFitArgs, got %T", pc.Args)
				}
				args.IgnoredResources = ignoredExtendedResources
				found = true
				break
			}
		}
		if !found {
			return nil, fmt.Errorf("can't find NodeResourcesFitArgs in plugin config")
		}

	##	如果有可忽略的拓展调度器，遍历所有的调度配置文件
	##	如果找到含有NodeResourcesFit插件的配置文件，会将可忽略的拓展调度器包含其中
	##	反之，如果无法找到相应插件或者拓展调度器的格式错误，程序会报错

	##	NodeResourcesFit 插件的主要功能是检查节点的资源是否满足 Pod 的需求。
	##	它通过比较 Pod 请求的资源和节点上可用的资源来决定一个节点是否适合该 Pod。
	##	这样可以确保调度器在不使用拓展调度器的情况下，也能对 Pod 进行节点选择

	##	不讲不可忽略调度器放置入NodeResourcesFit的原因
	##	NodeResourcesFit负责评估大部分情况下调度器对节点资源的打分
	##	不可忽略的拓展调度器往往负责特殊情况，放入NodeResourcesFit会导致增加消耗
	##	作为独立插件接入调度器也更方便对于这些不可忽略的拓展调度器进行配置与维护
	
	}
	return fExtenders, nil
}

##	这里便是扩展器的具体实现方法


type FailureHandlerFn func(ctx context.Context, fwk framework.Framework, podInfo *framework.QueuedPodInfo, status *framework.Status, nominatingInfo *framework.NominatingInfo, start time.Time)

##	FailureHandlerFn指定了在调度失败时应该执行的回调函数的格式
##	用于处理那些由于各种原因未能成功调度到节点上的 Pods。
##	ctx context.Context：
##	用于传递跨 API 边界和进程间的请求范围的元数据、取消信号以及截止时间。在调度的上下文中，它允许这些函数响应取消信号，优雅地中断处理过程。
##	fwk framework.Framework：
##	表示调度器框架的接口，提供了访问调度器内部操作的方法，如获取调度器的配置、调用特定的插件等。通过这个接口，失败处理函数可以利用调度器框架提供的工具和功能。
##	podInfo framework.QueuedPodInfo：
##	包含了队列中 Pod 的信息的结构体，如 Pod 的元数据和状态。这个信息用于帮助诊断为什么 Pod 调度失败，并可能被用来决定后续的重试或其他补救措施。
##	status framework.Status：
##	表示调度尝试结果的结构体。它包括了调度失败的原因、错误代码等详细信息。这些信息对于决定如何处理调度失败至关重要。
##	nominatingInfo framework.NominatingInfo：
##	包含了可能会影响 Pod 调度决策的额外信息，如推荐的节点或调度时考虑的特定细节。这允许失败处理逻辑根据调度时的情况来做出更精细的决策。
##	start time.Time：
##	记录了调度尝试开始的时间。这可以用于计算调度尝试耗费的时间，分析性能问题或确定调度超时。

func unionedGVKs(queueingHintsPerProfile internalqueue.QueueingHintMapPerProfile) map[framework.GVK]framework.ActionType {
	gvkMap := make(map[framework.GVK]framework.ActionType)
	for _, queueingHints := range queueingHintsPerProfile {
		for evt := range queueingHints {
			if _, ok := gvkMap[evt.Resource]; ok {
				gvkMap[evt.Resource] |= evt.ActionType
			} else {
				gvkMap[evt.Resource] = evt.ActionType
			}
		}
	}
	return gvkMap
}

##	这个程序会返回一个包含所有资源类型和相应动作类型的映射。
##	集群会更具这个映射来配置调度器，使其知道在哪些事件发生时需要重新考虑队列中的 Pods。

// newPodInformer creates a shared index informer that returns only non-terminal pods.
// The PodInformer allows indexers to be added, but note that only non-conflict indexers are allowed.
func newPodInformer(cs clientset.Interface, resyncPeriod time.Duration) cache.SharedIndexInformer {
	selector := fmt.Sprintf("status.phase!=%v,status.phase!=%v", v1.PodSucceeded, v1.PodFailed)
	tweakListOptions := func(options *metav1.ListOptions) {
		options.FieldSelector = selector
	}
	informer := coreinformers.NewFilteredPodInformer(cs, metav1.NamespaceAll, resyncPeriod, cache.Indexers{}, tweakListOptions)

	// Dropping `.metadata.managedFields` to improve memory usage.
	// The Extract workflow (i.e. `ExtractPod`) should be unused.
	trim := func(obj interface{}) (interface{}, error) {
		if accessor, err := meta.Accessor(obj); err == nil {
			if accessor.GetManagedFields() != nil {
				accessor.SetManagedFields(nil)
			}
		}
		return obj, nil
	}
	informer.SetTransform(trim)
	return informer
}

##	这个函数会创建一个用于监控 Pod 状态的Informer
##	调度器会使用这个 Informer 来监听 Pod 状态的变化，以便于实时做出调度决策
